<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"vikingstudyhard.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="通过同义词替换生成对抗样本">
<meta property="og:type" content="article">
<meta property="og:title" content="【论文精读】Adversarial Example For Natural Language Classification Problems">
<meta property="og:url" content="https://vikingstudyhard.github.io/2018/09/28/Paper_AdversarialExampleNLC/index.html">
<meta property="og:site_name" content="加载起来特别特别慢的小博客">
<meta property="og:description" content="通过同义词替换生成对抗样本">
<meta property="og:locale">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/Paper_AdversarialExampleNLC/6CFC125A08797BF761A8578F6ACE39CA.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/Paper_AdversarialExampleNLC/7A28C1E9EFD1F6938A92887DEE5C730F.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/Paper_AdversarialExampleNLC/B52E3CF25D30702E9DA3C9BF3D50345F.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/Paper_AdversarialExampleNLC/FFDC46C33F31F4F0D19A6B37C3DDB42F.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/Paper_AdversarialExampleNLC/D0E73A3410EEFFD090FFEB03753841A6.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/Paper_AdversarialExampleNLC/015820C10EECDFCF356B15D2F443B38E.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/Paper_AdversarialExampleNLC/A1C1673A497E4F0AEA85E4A2EEC60273.jpg">
<meta property="article:published_time" content="2018-09-27T16:00:00.000Z">
<meta property="article:modified_time" content="2021-05-11T06:56:58.510Z">
<meta property="article:author" content="VikingWang">
<meta property="article:tag" content="论文精读">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://vikingstudyhard.github.io/images/Paper_AdversarialExampleNLC/6CFC125A08797BF761A8578F6ACE39CA.jpg">

<link rel="canonical" href="https://vikingstudyhard.github.io/2018/09/28/Paper_AdversarialExampleNLC/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>【论文精读】Adversarial Example For Natural Language Classification Problems | 加载起来特别特别慢的小博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">加载起来特别特别慢的小博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">图片墙第一弹第二弹欢迎惠顾！</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-photo">

    <a href="/photo/" rel="section"><i class="fa fa-camera fa-fw"></i>photo</a>

  </li>
        <li class="menu-item menu-item-photo2">

    <a href="/photo2/" rel="section"><i class="fa fa-camera fa-fw"></i>photo2</a>

  </li>
        <li class="menu-item menu-item-friends">

    <a href="/friends/" rel="section"><i class="fa fa-rocket fa-fw"></i>friends</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://vikingstudyhard.github.io/2018/09/28/Paper_AdversarialExampleNLC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.JPG">
      <meta itemprop="name" content="VikingWang">
      <meta itemprop="description" content="这是一个需要耐心等着加载的博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="加载起来特别特别慢的小博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【论文精读】Adversarial Example For Natural Language Classification Problems
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-28 00:00:00" itemprop="dateCreated datePublished" datetime="2018-09-28T00:00:00+08:00">2018-09-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-05-11 14:56:58" itemprop="dateModified" datetime="2021-05-11T14:56:58+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI-testing/" itemprop="url" rel="index"><span itemprop="name">AI testing</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI-testing/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/" itemprop="url" rel="index"><span itemprop="name">文本对抗样本</span></a>
                </span>
            </span>

          
            <div class="post-description">通过同义词替换生成对抗样本</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="论文梳理"><a href="#论文梳理" class="headerlink" title="论文梳理"></a>论文梳理</h1><p>Anonymous authors. Paper under double-blind review. Under review as a conference paper at ICLR 2018. </p>
<h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><h3 id="对抗样本-adversarial-example"><a href="#对抗样本-adversarial-example" class="headerlink" title="对抗样本 adversarial example"></a>对抗样本 adversarial example</h3><h4 id="出处"><a href="#出处" class="headerlink" title="出处"></a>出处</h4><p>Christian Szegedy等人在2014年发表的文章：<em>Intriguing properties of neural networks</em><br>在数据集中通过故意添加细微的干扰所形成的输入样本，导致模型以高置信度给出一个错误的输出。<br><img src="/images/Paper_AdversarialExampleNLC/6CFC125A08797BF761A8578F6ACE39CA.jpg" alt="IMAGE"><br>如上样本x的label为熊猫，在对x添加部分干扰后，在人眼中仍然分为熊猫，但对深度模型，却将其错分为长臂猿，且给出了高达99.3%的置信度。</p>
<h4 id="原因简述"><a href="#原因简述" class="headerlink" title="原因简述"></a>原因简述</h4><p>以 $y = W^T \times  X$ 举例($W$是权重，$X$是输入)。如果 $X’ = X + t$，$t$ 为干扰，$W^T \times  X’ = W^T \times  X + W^T \times  t$，也就是多出一个 $W^T \times  t$ 项，$W$ 和 $t$ 维数很大时，即使很小扰动，累加起来也很可观。</p>
<h3 id="自然语言分类中的对抗干扰-adversarial-perturbations"><a href="#自然语言分类中的对抗干扰-adversarial-perturbations" class="headerlink" title="自然语言分类中的对抗干扰 adversarial perturbations"></a>自然语言分类中的对抗干扰 adversarial perturbations</h3><p>实验分类：垃圾邮件分类、情感分析、虚假新闻检测<br>主要思想：同义词替换，而不改变句子原义<br>如下图所示<br><img src="/images/Paper_AdversarialExampleNLC/7A28C1E9EFD1F6938A92887DEE5C730F.jpg" alt="IMAGE"></p>
<h3 id="发现"><a href="#发现" class="headerlink" title="发现"></a>发现</h3><ol>
<li>机器学习存在漏洞</li>
<li>分类算法有局限性</li>
</ol>
<p>##二、背景<br>分类器是映射 $f:\chi \rightarrow  \Upsilon $ ，其中输入 $x \in \chi$，标签 $y \in \Upsilon$，且标签来自K个类，即 $\Upsilon = \left { y_{1}, y_{2},…,y_{K} \right }$。<br>分类器 $f$ 是个深度神经网络或者线性模型，它会为输入 $x$ 在每个类 $y_{k}$ 中的可能性做出评分值 $f_{y_{k}}(x)$，并将评分最高的那个类作为输出。</p>
<h3 id="图片分类器"><a href="#图片分类器" class="headerlink" title="图片分类器"></a>图片分类器</h3><p>对正常输入 $x$ 做扰动后得到 $x’$，将其送入分类器，得到错误的类别 $y’$，这一过程可以如下定能够如下定义：$$f(x’)=y’ and  \left | x-x’ \right |\leq \varepsilon $$</p>
<p>优化为：<br>$$ max_{x’}J(x’)    s.t.    \left | x-x’ \right |\leq \varepsilon $$<br>其中 $J(x’)$ 即为评分值 $f_{y_{k}}(x’)$</p>
<h3 id="自然语言分类"><a href="#自然语言分类" class="headerlink" title="自然语言分类"></a>自然语言分类</h3><p>具有n-gram特性的线性分类器往往具有非常好的性能。循环神经网络RNN与卷积神经网络CNN在文本分类方面都有良好效果。<br>###自然语言分类与图形分类的区别</p>
<ol>
<li>自然语言是离散的，输入的x为字符或者单词。</li>
<li>自然语言维度更高，与词汇量成比例。</li>
<li>自然语言比图片像素具有更高的层次，因为原始的单词比原始的像素带有更多信息。</li>
</ol>
<p>##三、自然语言分类器的对抗样本</p>
<p>产生文本分类器中的对抗样本有两个难点：</p>
<ol>
<li>话语之间没有度量的简单概念(难以定义一个无法察觉的“扰动”)。</li>
<li>离散输入不适合基于梯度的方法，因此需要新的优化算法。</li>
</ol>
<h3 id="Altered-Adversarial-Examples"><a href="#Altered-Adversarial-Examples" class="headerlink" title="Altered Adversarial Examples"></a>Altered Adversarial Examples</h3><p>对于一个分类器f，如果存在一个x’使得：<br>$$f(x’)=y’ and  \left | x-x’ \right |\leq \gamma  $$</p>
<p>就说 $x’$ 是一个对抗改动（adversarial alteration）。</p>
<p>其中函数 $c$ 是一种约束 $c:\chi  \times  \chi \rightarrow \mathbb{R}_{+ }^{L}$ ，$\gamma $是一个边界向量（a vector of bounds），且 $ \gamma \in \mathbb{R}^{L} $。 $c$ 和 $\gamma $ 通过约束 $L \leq 1$ 捕捉“不可察觉的改变”的概念。 </p>
<h3 id="Adversarial-Examples-for-Natural-Language-Classification"><a href="#Adversarial-Examples-for-Natural-Language-Classification" class="headerlink" title="Adversarial Examples for Natural Language Classification"></a>Adversarial Examples for Natural Language Classification</h3><p>在自然语言环境中，我们希望改动的样本 $x’$ 能与原样本 $x$ 保存相同的意思。为此我们提出了一种特殊的限制函数 $c(x-x’)$，使两种表述有相同的含义并保持相同的句法特征（syntactic properties），例如，写作风格应该保持相似。具体来说，函数c由两个约束组成，分别在两个层次上捕捉句子的相似性。</p>
<h4 id="语义层次（semantic-similarity）"><a href="#语义层次（semantic-similarity）" class="headerlink" title="语义层次（semantic similarity）"></a>语义层次（semantic similarity）</h4><p>用思考向量（thought vector）（参考《Efficient estimation of word representations in vector space》）的概念来捕获话语的含义。思考向量可以看作是从句子到向量空间的映射，其中具有相似含义的句子彼此接近。在此意义下，约束被定义为：<br>$$ \left | v-v’ \right |<em>{2}  \leq \gamma</em>{1}  $$<br>其中 $v$ 和 $v’$ 是与 $x$ 和 $x’$ 对应的思考向量，$ \gamma_{1}$ 是超参数。<br>本文中将思考向量设为单个词的向量的平均值。</p>
<h4 id="句法层次（syntactic-similarity）"><a href="#句法层次（syntactic-similarity）" class="headerlink" title="句法层次（syntactic similarity）"></a>句法层次（syntactic similarity）</h4><p>通常thought vector不能捕捉到句法的正确性。比如将对句子中的单词重新排序，可以得到相同的平均词向量。为此，加上句法约束<br>$$ \left | log P(x’)  -  log P(x) \right | &lt;  \gamma <em>{2} $$<br>它依赖于语言模型 $ P : \chi \rightarrow \left[0, 1\right ]$ ，我们要保证扰动和原始示例之间的语言模型概率相似（如果$x$  是一个不合语法的句子，即使用不正确的英语，那么 $x</em>{1}$ 应该保持相似的正确性水平）。</p>
<h3 id="Greedy-Construction-of-Altered-Adversarial-Examples"><a href="#Greedy-Construction-of-Altered-Adversarial-Examples" class="headerlink" title="Greedy Construction of Altered Adversarial Examples"></a>Greedy Construction of Altered Adversarial Examples</h3><p>优化<br>$$ max_{x’}J(x’)    s.t.   c(x-x’) \leq \gamma $$</p>
<p>其中，$ J(x’) $ 就是 $f_{y’}(x’)$ 且 $y’ \neq  y$<br>Algorithm 1: Greedy Optimization Strategy for Finding Adversarial Examples</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Data: Datapoint x, termination threshold τ , neighborhood size N, parameters γ<span class="number">1</span>, γ<span class="number">2</span>, δ.</span><br><span class="line">We initialize the algorithm at the uncorrupted data point: x<span class="number">&#x27;</span> ← x ;</span><br><span class="line"><span class="function"><span class="keyword">while</span> Objective is below the threshold <span class="title">J</span><span class="params">(x<span class="string">&#x27;) &lt; τ and fraction of words replaced is less than δ do</span></span></span></span><br><span class="line"><span class="string"><span class="params"><span class="function">    Create a working set W = ∅ ;</span></span></span></span><br><span class="line"><span class="string"><span class="params"><span class="function">    for each word w in x do</span></span></span></span><br><span class="line"><span class="string"><span class="params"><span class="function">        for each word w¯ among the N closest to w and different from w do</span></span></span></span><br><span class="line"><span class="string"><span class="params"><span class="function">            substitute w&#x27;</span> with w¯ to get x¯ <span class="keyword">and</span> <span class="keyword">if</span> x¯ satisfies ∣logP(x′)−logP(x)∣&lt;γ<span class="number">2</span> , then W ← W ∪ &#123;x<span class="string">&#x27;&#125;;</span></span></span></span><br><span class="line"><span class="string"><span class="params"><span class="function">    Choose highest scoring world replacement  x&#x27;</span> ← arg max x¯∈W J(x¯) <span class="keyword">or</span> <span class="keyword">if</span> W = ∅, then <span class="keyword">break</span> ;</span></span></span><br><span class="line"><span class="params"><span class="function"><span class="keyword">return</span> x<span class="string">&#x27;;</span></span></span></span><br></pre></td></tr></table></figure>


<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>对句子中某一单词找 $N$ 个近义词来替换，并且满足句法层次一致，就把新的句子加入 $W$ 集合里。对每个单词都进行这样的操作，最后返回评分最高的句子。</p>
<h4 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h4><p>算法输入（algorithm inputs）</p>
<p>需要一个目标分类器 $f$，算法1通过优化目标 $J$ 将 $x$ 转化为 $x’$。假设 $x$ 是一组 $n$ 个离散符号（即单词,word），用 $w_{i}$  表示，$i=1,2,…,n$。此算法可以扩展到相似的通常的离散问题。</p>
<p>优化策略（optimization strategies）</p>
<p>首先我们定义一个边界向量 $\delta$，例如使其满足 $\sum_{i=1}^{n}\mathbb{I}{w_{i}\neq w_{i}’}\leq \delta \cdot n$ ，这可以使我们排除掉一些显然不满足条件的对抗改动。或者，设置一个关于目标的最小阈值 $\gamma $，例如目标标签分数的最小期望值，在我们达到这个最小阈值时停止算法。</p>
<p>单词替换（word repalcement）</p>
<p>我们用一个合适的词向量空间中的最近邻居（nearest neighbors）替换单词，并考虑N个最近的邻居。这些最近邻居单词很可能出现在相同的文本环境中。为了确保更换的也是同义词,我们使用的GloVE词向量（GloVE word vectors）进行后期处理（出自《 Counter-fitting word vectors to linguistic constraints》）。这确保了向量满足已知的同义词关系所施加的语言约束，并确保具有相似含义的单词在向量空间中彼此接近。<br><img src="/images/Paper_AdversarialExampleNLC/B52E3CF25D30702E9DA3C9BF3D50345F.jpg" alt="IMAGE"></p>
<p>##四、实验</p>
<h3 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h3><p>在垃圾邮件分类(Spam filtering)、情感分析(Sentiment analysis)、虚假新闻检测(Fake news detection)三种自然语言分类任务中使用对抗样本。<br><img src="/images/Paper_AdversarialExampleNLC/FFDC46C33F31F4F0D19A6B37C3DDB42F.jpg" alt="IMAGE"><br>将训练集中的10%用作验证集，所有对抗样本通过测试集产生、评估。同样我们为每一项任务，在训练集中训练一个卦语言模型（trigram language model）。并且实例化 中的词向量语义约束。</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>####朴素贝叶斯<br>naive Bayes：将每个文档转换为一个词袋（bag-of words）表示形式，然后按照Wang &amp; Manning(2012)的方法，将单词特征进行二值化，并使用一个多项式模型进行分类。<br>####长短记忆网路<br>long short-term memory：建立了一个拥有512个隐藏神经元的单层 LSTM 。在输入LSTM前先进行 word2vec 词嵌入，变成300维向量。然后，我们对LSTM在每个时间步上的输出进行平均，得到一个特征向量，用于最后的逻辑回归预测情感。<br>####浅单词级卷积网络<br>shallow word-level convolutional networks：用嵌入层(如在LSTM中)训练一个CNN，一个时间卷积层，然后是最大池化，最后用一个全连接层进行分类。<br>####深字符集卷积网络<br>deep character-level convolutional networks：<br>###主要实验<br>我们手动选择了优化设置，从而在对抗样本的强度和一致性之间做出了合理的权衡。<br>在所有实验中，我们令阈值 $\tau =0.7$，临近大小 $N=15$，参数 $\gamma_{1}=0.2$ 且 $ \delta_{1}=0.5$。在情感分析和假消息检测中令 $\gamma_{2}=2$，在垃圾信息检测中令 $\gamma_{2}=\infty $。我们还比较了用随机抽样代替算法1中的$arg max$得到的随机扰动。发现模型对于随机扰动的抵抗能力很强。</p>
<p><img src="/images/Paper_AdversarialExampleNLC/D0E73A3410EEFFD090FFEB03753841A6.jpg" alt="IMAGE"></p>
<h3 id="人类评估"><a href="#人类评估" class="headerlink" title="人类评估"></a>人类评估</h3><p>通过Amazon Mechanical Turk上的人体实验验证了我们的示例的质量和一致性。</p>
<p>首先，对100个随机测试集示例进行二次抽样，并让人类评估人员为原始数据点及其对抗性更改版本分配标签（例如正面或负面评论）。我们对每个查询平均了五种不同评估的意见。我们发现人类评估者在两种类型的投入上都达到了类似的准确度，这表明我们的对抗性改变足以保证关键语义能够得到人类的认可。人的准确性通常低于算法的准确性：虚假新闻任务本质上是困难的，而非垃圾邮件通常被错误分类，因为“ham”电子邮件没有标准定义;在情绪分析中，两种准确度都在合理的误差范围内。</p>
<p>接下来，我们要求人类注释者按照1到5的等级对同一组示例的“书写质量”进行评级，其中五个最高质量的（可能由人类生成）；一个质量最低的（可能是通过机器生成的）。表5显示人类倾向于为两组样本分配相似的分数。尽管我们的对抗性示例并未完美形成，但这些结果表明它们的质量与原始示例相当（其中还包含多个拼写和语法错误）。<br><img src="/images/Paper_AdversarialExampleNLC/015820C10EECDFCF356B15D2F443B38E.jpg" alt="IMAGE"></p>
<h3 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h3><p>我们发现我们的对抗性示例表现出三种错误：句法，语义和事实。<br>####句法错误<br>句法错误是不合语法的单词替换，包括</p>
<ol>
<li><p>将“isis claim responsibility for shooting（isis声称对射击负责）”替换为“isis petition responsibility for shooting（isis请愿射击的责任）”<br>这个错误是由于多个单词含义。claim和petition都有请求的意思。</p>
</li>
<li><p>将“never before has an fbi director”替换为“never until has an fbi director”<br>这个错误是由于单词不相关（并且在单词向量空间中很远）。</p>
</li>
</ol>
<p>####语义错误<br>当句子的含义被改变时，就会出现语义错误。</p>
<ol>
<li>大多数情况下，这是由于多个词的意义相同。例如，“isis claim responsibility for shooting”到“isis claim responsibility for filming” </li>
<li>词语嵌入错误。例如，“iisis claim responsibility for ceasefire”。</li>
</ol>
<p>####事实错误<br>当句子变得明显错误时，事实错误是一种特殊情况。当“三月十六日星期一”改为“三月十六日星期四”或“联邦调查局副局长詹姆斯卡尔斯特罗姆”改为“五角大楼助理导演詹姆斯卡尔斯特罗姆”，或“共和党人支持特朗普”改为“支持奥巴马的共和党人”时。<br>这些可能不是假评论或假新闻的问题，并且可以通过专门技术来补救，例如，通过performing Named Entity Recognition （执行命名实体识别）。</p>
<h3 id="传递性"><a href="#传递性" class="headerlink" title="传递性"></a>传递性</h3><p>图像分类模型的一个有趣的特性是，为一个分类器生成的对抗性示例可能被其他分类器错误分类。 我们还检查了对抗文本是否在四个模型之间转移，重点关注Yelp数据集。 如表6所示，模型之间存在中等程度的可转移性。</p>
<p><img src="/images/Paper_AdversarialExampleNLC/A1C1673A497E4F0AEA85E4A2EEC60273.jpg" alt="IMAGE"></p>
<p>三个单词级别模型（NB，LSTM，WCNN）的对抗性示例也没有对字符级别深度CNN和其他单词级别模型进行推广，这表明输入表示（字符或单词）的选择是一个因素，这会影响可转移性。</p>
<p>#探究</p>
<h2 id="统计语言模型（Statistical-Language-Model）"><a href="#统计语言模型（Statistical-Language-Model）" class="headerlink" title="统计语言模型（Statistical Language Model）"></a>统计语言模型（Statistical Language Model）</h2><blockquote>
<p>参考 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/f-young/p/7906451.html">https://www.cnblogs.com/f-young/p/7906451.html</a></p>
</blockquote>
<p>语言模型就是用来计算一个句子的概率的模型，即P(W1,W2,…Wk)。利用语言模型，可以确定哪个词序列的可能性更大，或者给定若干个词，可以预测下一个最可能出现的词语。举个音字转换的例子来说，输入拼音串为nixianzaiganshenme，对应的输出可以有多种形式，如你现在干什么、你西安再赶什么、等等，那么到底哪个才是正确的转换结果呢，利用语言模型，我们知道前者的概率大于后者，因此转换成前者在多数情况下比较合理。再举一个机器翻译的例子，给定一个汉语句子为李明正在家里看电视，可以翻译为Li Ming is watching TV at home、Li Ming at home is watching TV、等等，同样根据语言模型，我们知道前者的概率大于后者，所以翻译成前者比较合理。</p>
<h3 id="计算一个句子的概率"><a href="#计算一个句子的概率" class="headerlink" title="计算一个句子的概率"></a>计算一个句子的概率</h3><p>给定句子（词语序列）S=W1,W2,…,Wk，它的概率可以表示为：<br>$$P(S)=P(W_{1},W_{2},W_{3},…,W_{k}) =p(W_{1})p(W_{2}|W_{1})P(W_{3}|W_{1}W_{2})…p(W_{k}|W_{1},W_{2},…,W_{k}) $$<br>由于上式中的参数过多，因此需要近似的计算方法。常见的方法有n-gram模型方法、决策树方法、最大熵模型方法、最大熵马尔科夫模型方法、条件随机域方法、神经网络方法，等等。</p>
<h3 id="n-gram语言模型"><a href="#n-gram语言模型" class="headerlink" title="n-gram语言模型"></a>n-gram语言模型</h3><p>n-gram模型也称为n-1阶马尔科夫模型，它有一个有限历史假设：当前词的出现概率仅仅与前面n-1个词相关。<br>$$ P(S) =P(W_{1},W_{2},W_{3},…,W_{k}) \approx  \prod_{i=1}^{k}P(W_{i}|W_{1},…,W_{i-1}) $$<br>当n取1、2、3时，n-gram模型分别称为unigram、bigram和trigram语言模型。n-gram模型的参数就是条件概率 $P(W_{i}|W_{i-n+1},…,W_{i-1})$ 。假设词表的大小为100,000，那么n-gram模型的参数数量为$100,000^{n}$。n越大，模型越准确，也越复杂，需要的计算量越大。最常用的是bigram，其次是unigram和trigram，n取≥4的情况较少。</p>
<p>假设上面的n不取很长，而只取2个，那么就可以大大减少计算量。此时，假设一个词$w_{i}$出现的概率只与它前面的$w_{i−1}$有关，这种假设称为1阶马尔科夫假设。</p>
<p>$$P(w_1,w_2,…,w_n) \approx P(w_1)P(w_2\vert w_1)P(w_3\vert w_2)…P(w_n\vert w_{n-1})$$</p>
<p>那么，接下来的问题就变成了估计条件概率 $P(w_{i}\vert w_{i−1})$，根据它的定义 $P(w_i\vert w_{i-1}) = \frac{P(w_i,w_{i-1})}{P(w_{i-1})}$<br>当样本量很大的时候，基于大数定律，一个短语或者词语出现的概率可以用其频率来表示，即<br>$$P(w_i,w_{i-1}) \approx \frac{count(w_i,w_{i-1})}{count(<em>)}$$<br>$$P(w_{i-1}) \approx \frac{count(w_{i-1})}{count(</em>)}$$<br>其中，count(i)表示词i出现的次数，count表示语料库的大小。<br>那么<br>$$ P(w_i\vert w_{i-1}) = \frac{P(w_i,w_{i-1})}{P(w_{i-1})} \approx \frac{count(w_i,w_{i-1})}{count(w_{i-1})}$$</p>
<h3 id="高阶语言模型"><a href="#高阶语言模型" class="headerlink" title="高阶语言模型"></a>高阶语言模型</h3><p>考虑以下两个问题，对于二元模型：</p>
<ul>
<li>如果此时 $count(w_i,w_{i-1})=0$，是否可以说 $P(w_i\vert w_{i-1})=0$ ?</li>
<li>如果此时 $count(w_i,w_{i-1})=count(w_{i-1})$，是否可以说 $P(w_i\vert w_{i-1})=1$ ?</li>
</ul>
<p>显然不能。<br>古德图灵估计的原理：</p>
<blockquote>
<p>对于没有看见的事件，我们不能认为他发生的概率就是0，因此从概率的总量中，分配一个很小的比例给这些没有看见的事件。这样一来，看见的那些事件的概率就要小于1了，因此，需要将所有看见的事件的概率调小一点。至于小多少，要根据“越是不可信的统计折扣越多”的方法进行。</p>
</blockquote>
<h3 id="神经网络语言模型"><a href="#神经网络语言模型" class="headerlink" title="神经网络语言模型"></a>神经网络语言模型</h3><p>n-gram模型不足</p>
<ul>
<li>没有考虑上下文中更多的词提供的信息</li>
<li>没有考虑词与此之间的相似性</li>
</ul>
<p>神经网络语言模型可以概括为以下三点：</p>
<ul>
<li>将词汇表中的每个词表示成一个在m维空间里的实数形式的分布式特征向量</li>
<li>使用序列中词语的分布式特征向量来表示连接概率函数</li>
<li>同时学习特征向量和概率函数的参数</li>
</ul>
<p>特征向量表示词的不同特征：<br>每一个词都是向量空间内的一个点。特征的个数通常都比较小，比如30，60或者100，远远小于词汇表的长度。概率函数是在给定一个词前面的若干词的情况下，该词出现的条件概率。调整概率函数的参数，使得训练集的对数似然达到最大。每个词的特征向量是通过训练得到的，也可以用先验知识进行初始化。</p>
<h2 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h2><p>参考</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/mawenqi0729/article/details/80698350">https://blog.csdn.net/mawenqi0729/article/details/80698350</a></p>
</blockquote>
<h3 id="词的两种表示方式"><a href="#词的两种表示方式" class="headerlink" title="词的两种表示方式"></a>词的两种表示方式</h3><p>one-hot representation 和 distribution representation。</p>
<ol>
<li>离散表示<br>one-hot representation把每个词表示为一个长向量。这个向量的维度是词表大小，向量中只有一个维度的值为1，其余维度为0，这个维度就代表了当前的词。<br>例如：<br>苹果 [0，0，0，1，0，0，0，0，0，……]<br>one-hot representation相当于给每个词分配一个id，这就导致这种表示方式不能展示词与词之间的关系。另外，one-hot representation将会导致特征空间非常大，但也带来一个好处，就是在高维空间中，很多应用任务线性可分。</li>
<li>分布式表示<br>word embedding指的是将词转化成一种分布式表示，又称词向量。分布<br>式表示将词表示成一个定长的连续的稠密向量。</li>
</ol>
<p>分布式表示优点:<br>(1)词之间存在相似关系：<br>是词之间存在“距离”概念，这对很多自然语言处理的任务非常有帮助。<br>(2)包含更多信息：<br>词向量能够包含更多信息，并且每一维都有特定的含义。在采用one-hot特征时，可以对特征向量进行删减，词向量则不能。</p>
<h3 id="生成词向量的方法"><a href="#生成词向量的方法" class="headerlink" title="生成词向量的方法"></a>生成词向量的方法</h3><p>生成词向量的方法方法都依照一个思想：任一词的含义可以用它的周边词来表示。<br>生成词向量的方式可分为：基于统计的方法和基于语言模型(language model)的方法。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/" rel="tag"># 论文精读</a>
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/06/28/Prose1/" rel="prev" title="【Prose】01《从今以后，仔仔细细地过》">
      <i class="fa fa-chevron-left"></i> 【Prose】01《从今以后，仔仔细细地过》
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/10/27/Paper_AdversarialTrainingMethodsForSemi-SupervisedTextClassification/" rel="next" title="【论文精读】Adversarial Training Methods for Semi-Supervised Text Classification">
      【论文精读】Adversarial Training Methods for Semi-Supervised Text Classification <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86"><span class="nav-number">1.</span> <span class="nav-text">论文梳理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.1.</span> <span class="nav-text">一、介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC-adversarial-example"><span class="nav-number">1.1.1.</span> <span class="nav-text">对抗样本 adversarial example</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%87%BA%E5%A4%84"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">出处</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8E%9F%E5%9B%A0%E7%AE%80%E8%BF%B0"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">原因简述</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%88%86%E7%B1%BB%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E5%B9%B2%E6%89%B0-adversarial-perturbations"><span class="nav-number">1.1.2.</span> <span class="nav-text">自然语言分类中的对抗干扰 adversarial perturbations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%91%E7%8E%B0"><span class="nav-number">1.1.3.</span> <span class="nav-text">发现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">1.1.4.</span> <span class="nav-text">图片分类器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%88%86%E7%B1%BB"><span class="nav-number">1.1.5.</span> <span class="nav-text">自然语言分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Altered-Adversarial-Examples"><span class="nav-number">1.1.6.</span> <span class="nav-text">Altered Adversarial Examples</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adversarial-Examples-for-Natural-Language-Classification"><span class="nav-number">1.1.7.</span> <span class="nav-text">Adversarial Examples for Natural Language Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%AD%E4%B9%89%E5%B1%82%E6%AC%A1%EF%BC%88semantic-similarity%EF%BC%89"><span class="nav-number">1.1.7.1.</span> <span class="nav-text">语义层次（semantic similarity）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%A5%E6%B3%95%E5%B1%82%E6%AC%A1%EF%BC%88syntactic-similarity%EF%BC%89"><span class="nav-number">1.1.7.2.</span> <span class="nav-text">句法层次（syntactic similarity）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Greedy-Construction-of-Altered-Adversarial-Examples"><span class="nav-number">1.1.8.</span> <span class="nav-text">Greedy Construction of Altered Adversarial Examples</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%9D%E8%B7%AF"><span class="nav-number">1.1.8.1.</span> <span class="nav-text">思路</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90"><span class="nav-number">1.1.8.2.</span> <span class="nav-text">算法分析</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A1"><span class="nav-number">1.1.9.</span> <span class="nav-text">任务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.1.10.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%BA%E7%B1%BB%E8%AF%84%E4%BC%B0"><span class="nav-number">1.1.11.</span> <span class="nav-text">人类评估</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%94%99%E8%AF%AF%E5%88%86%E6%9E%90"><span class="nav-number">1.1.12.</span> <span class="nav-text">错误分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%A0%E9%80%92%E6%80%A7"><span class="nav-number">1.1.13.</span> <span class="nav-text">传递性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88Statistical-Language-Model%EF%BC%89"><span class="nav-number">1.2.</span> <span class="nav-text">统计语言模型（Statistical Language Model）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E4%B8%80%E4%B8%AA%E5%8F%A5%E5%AD%90%E7%9A%84%E6%A6%82%E7%8E%87"><span class="nav-number">1.2.1.</span> <span class="nav-text">计算一个句子的概率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#n-gram%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.2.2.</span> <span class="nav-text">n-gram语言模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E9%98%B6%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.2.3.</span> <span class="nav-text">高阶语言模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.2.4.</span> <span class="nav-text">神经网络语言模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%8D%E5%90%91%E9%87%8F"><span class="nav-number">1.3.</span> <span class="nav-text">词向量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%8D%E7%9A%84%E4%B8%A4%E7%A7%8D%E8%A1%A8%E7%A4%BA%E6%96%B9%E5%BC%8F"><span class="nav-number">1.3.1.</span> <span class="nav-text">词的两种表示方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">1.3.2.</span> <span class="nav-text">生成词向量的方法</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="VikingWang"
      src="/images/avatar.JPG">
  <p class="site-author-name" itemprop="name">VikingWang</p>
  <div class="site-description" itemprop="description">这是一个需要耐心等着加载的博客</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">39</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">VikingWang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
