<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"vikingstudyhard.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="第二章 Linear regression with one variable">
<meta property="og:type" content="article">
<meta property="og:title" content="【Andrew Ng】Machine Learning | 2 Linear regression with one variable">
<meta property="og:url" content="https://vikingstudyhard.github.io/2019/01/18/MachineLearning_AndrewNg_2/index.html">
<meta property="og:site_name" content="加载起来特别特别慢的小博客">
<meta property="og:description" content="第二章 Linear regression with one variable">
<meta property="og:locale">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/B9767009C0EA72B2F9416B1B7FB0DACE.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/FF8DAC215FBEB505C1D890ED2F50DA8C.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/4071B2EB60BF7550D894FC50C1A0077F.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/146F4D691533804D5236D130D3BF9BDF.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/329D21C545ED8110919FEFD6EA892951.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/F4B776C812DDB22DF7102EDAAA8B8A3E.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/316A35708F1C899F3CCF1AC03AFBA961.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/0B969E968913DA5B40B4BBF20E4D8BF7.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/5193979231F83D7C126B00D2EB3E41A0.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/7865D8C00D7C01E5776D62F012A24848.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/0AE007740ED8059A309EDBBEB6CF7E1B.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/A68D3BF267AA32989234D55DE40793A8.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/A6428B3E31F2C2ED50CED904124A04F6.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/60EBCF577626A935F9FEC321BE324440.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/11946CF34AF9427A65A82096AD9ADAF9.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/0EA4FB989F19304321231160FEC88A12.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/835A08F5195B5A9CFBB4307AF812A3C5.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/9B517673316F8E106BBC84BBA438184E.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/2D3855C9C6833650C417EEBF13B4B138.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/96EC02C8FF28F0AFEE43C3EDF51E8449.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/A2412BF9070241E428678E7797C242A4.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/D7374612F612FE96DB8B3C1C9AC0F33F.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/69DB3FC7ED45D339810C543A4AB0D8FC.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/6B66672C792589971296FDBC8640B2C0.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/731B07C8067BC6AF6B71C7A0BE4C4F5E.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/EA8EA41E04D01BCE2B944EB65CDB99EA.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/18C1EED4373417F265F9F4A4052E30B7.jpg">
<meta property="og:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/26BB6DF735AC62BC2FFAADD02564C522.jpg">
<meta property="article:published_time" content="2019-01-17T16:00:00.000Z">
<meta property="article:modified_time" content="2021-05-11T06:56:58.510Z">
<meta property="article:author" content="VikingWang">
<meta property="article:tag" content="Andrew Ng">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/B9767009C0EA72B2F9416B1B7FB0DACE.jpg">

<link rel="canonical" href="https://vikingstudyhard.github.io/2019/01/18/MachineLearning_AndrewNg_2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>【Andrew Ng】Machine Learning | 2 Linear regression with one variable | 加载起来特别特别慢的小博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">加载起来特别特别慢的小博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">图片墙第一弹第二弹欢迎惠顾！</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-photo">

    <a href="/photo/" rel="section"><i class="fa fa-camera fa-fw"></i>photo</a>

  </li>
        <li class="menu-item menu-item-photo2">

    <a href="/photo2/" rel="section"><i class="fa fa-camera fa-fw"></i>photo2</a>

  </li>
        <li class="menu-item menu-item-friends">

    <a href="/friends/" rel="section"><i class="fa fa-rocket fa-fw"></i>friends</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://vikingstudyhard.github.io/2019/01/18/MachineLearning_AndrewNg_2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.JPG">
      <meta itemprop="name" content="VikingWang">
      <meta itemprop="description" content="这是一个需要耐心等着加载的博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="加载起来特别特别慢的小博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【Andrew Ng】Machine Learning | 2 Linear regression with one variable
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-01-18 00:00:00" itemprop="dateCreated datePublished" datetime="2019-01-18T00:00:00+08:00">2019-01-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-05-11 14:56:58" itemprop="dateModified" datetime="2021-05-11T14:56:58+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-learning/" itemprop="url" rel="index"><span itemprop="name">Machine learning</span></a>
                </span>
            </span>

          
            <div class="post-description">第二章 Linear regression with one variable</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>第二章 Linear regression with one variable 单变量线性回归</p>
<h1 id="1-Model-representation-模型描述"><a href="#1-Model-representation-模型描述" class="headerlink" title="1 Model representation 模型描述"></a>1 Model representation 模型描述</h1><div align=center>
  <img width=567 src="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/B9767009C0EA72B2F9416B1B7FB0DACE.jpg" >
</div>

<p>有监督学习：数据中给出了每个样本对应的正确答案<br>回归问题：预测具体数值输出</p>
<p>预测房价</p>
<h2 id="训练集"><a href="#训练集" class="headerlink" title="训练集"></a>训练集</h2><div align=center>
  <img width=583 src="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/FF8DAC215FBEB505C1D890ED2F50DA8C.jpg" >
</div>

<p>$m$ 训练样本的数量<br>$x$ 输入变量/特征<br>$y$ 输出变量/要预测的目标变量<br>$(x,y)$ 一组训练样本<br>$(x^{(i)},y^{(i)})$ 第i组训练样本，i是index不是幂。</p>
<h2 id="假设函数-hypothesis-function"><a href="#假设函数-hypothesis-function" class="headerlink" title="假设函数 hypothesis function"></a>假设函数 hypothesis function</h2><div align=center>
  <img width=406 src="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/4071B2EB60BF7550D894FC50C1A0077F.jpg" >
</div>

<p>$ h_\theta (x) = \theta_0 + \theta_1x $ 缩写为 $h(x)$</p>
<div align=center>
  <img width=253 src="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/146F4D691533804D5236D130D3BF9BDF.jpg" >
</div>

<p>称为：<br>Linear regression with one variable 一元线性回归<br>Univariate linear regression 单变量线性回归</p>
<h1 id="2-Cost-function-代价函数"><a href="#2-Cost-function-代价函数" class="headerlink" title="2 Cost function 代价函数"></a>2 Cost function 代价函数</h1><h2 id="代价函数意义"><a href="#代价函数意义" class="headerlink" title="代价函数意义"></a>代价函数意义</h2><p>代价函数用来衡量假设函数的准确性。</p>
<p>假设函数中的 $\theta_0$ $\theta_1$ 称为模型参数，如何选择参数？</p>
<div align=center>
  <img width=314 src="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/329D21C545ED8110919FEFD6EA892951.jpg" >
</div>

<h2 id="代价函数公式推导"><a href="#代价函数公式推导" class="headerlink" title="代价函数公式推导"></a>代价函数公式推导</h2><p>最小化问题 minimize:<br>使每组样本的 $\left ( h_\theta (x) - y \right )^2$ 最小，即使下式最小<br>$\sum_{i=1}^{m}\left ( h_\theta (x^{(i)}) - y^{(i)} \right )^2$ ，也就是预测值和实际值的差的平方最小。</p>
<p>平均误差：上式乘以$ \frac{1}{m}$。</p>
<p>通常求这个值的一半，也就是乘以$ \frac{1}{2m}$。平均值减半是为了便于计算梯度下降，因为平方函数的导数项将抵消$ \frac{1}{2}$项。</p>
<p>找到使上式最小的$\theta_0$ $\theta_1$：$\underset{\theta_0 ,\theta_1}{minimize}\frac{1}{2m}\sum_{i=1}^{m}\left ( h_\theta (x^{(i)}) - y^{(i)} \right )^2$</p>
<h2 id="代价函数定义"><a href="#代价函数定义" class="headerlink" title="代价函数定义"></a>代价函数定义</h2><p>$$J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^{m}\left ( h_\theta (x^{(i)}) - y^{(i)} \right )^2$$<br>也称为平方误差函数 Squared error function、Mean squared error</p>
<p>目的是 $\underset{\theta_0 , \theta_1}{minimize}J(\theta_0,\theta_1)$</p>
<h1 id="3-Cost-function-intuition-I"><a href="#3-Cost-function-intuition-I" class="headerlink" title="3 Cost function intuition I"></a>3 Cost function intuition I</h1><h2 id="单一参数下代价函数与假设函数的关系"><a href="#单一参数下代价函数与假设函数的关系" class="headerlink" title="单一参数下代价函数与假设函数的关系"></a>单一参数下代价函数与假设函数的关系</h2><p><img src="/images/MachineLearning_AndrewNg_2/F4B776C812DDB22DF7102EDAAA8B8A3E.jpg" alt="IMAGE"><br>简化为 $\theta_0 = 0$</p>
<p><img src="/images/MachineLearning_AndrewNg_2/316A35708F1C899F3CCF1AC03AFBA961.jpg" alt="IMAGE"><br>对于(1,1)(2,2)(3,3)三个点，$h_\theta(x)=x$时正好经过三个点，此时$\theta_1 = 1$，$J(\theta_1) = 0$。</p>
<p><img src="/images/MachineLearning_AndrewNg_2/0B969E968913DA5B40B4BBF20E4D8BF7.jpg" alt="IMAGE"><br>当$\theta_1 = 0.5$时，$J(\theta_1) = 0.58$。</p>
<p><img src="/images/MachineLearning_AndrewNg_2/5193979231F83D7C126B00D2EB3E41A0.jpg" alt="IMAGE"><br>当$\theta_1 = 0$时，$J(\theta_1) = 2.3$。</p>
<p><img src="/images/MachineLearning_AndrewNg_2/7865D8C00D7C01E5776D62F012A24848.jpg" alt="IMAGE"><br>代价函数最小时，$\theta_1 = 1$。</p>
<p>最好的假设函数是使得散点到该线的平均垂直距离最小的函数。理想情况下，该线应该通过训练数据集的所有点。 </p>
<h1 id="4-Cost-function-intuition-II"><a href="#4-Cost-function-intuition-II" class="headerlink" title="4 Cost function intuition II"></a>4 Cost function intuition II</h1><h2 id="双参数下代价函数与假设函数的关系"><a href="#双参数下代价函数与假设函数的关系" class="headerlink" title="双参数下代价函数与假设函数的关系"></a>双参数下代价函数与假设函数的关系</h2><p>$J(\theta_0,\theta_1)$函数大致如下图形状</p>
<div align=center>
  <img width=465 src="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/0AE007740ED8059A309EDBBEB6CF7E1B.jpg" >
</div>

<p>转化为等高线图 contour plots/figures<br>同一条线上的点对应的$J(\theta_0,\theta_1)$值相同。<br><img src="/images/MachineLearning_AndrewNg_2/A68D3BF267AA32989234D55DE40793A8.jpg" alt="IMAGE"><br>$\theta_0 = 800 $ $\theta_1 = -0.15$时，$ h_\theta (x)$ 如上图所示。</p>
<p><img src="/images/MachineLearning_AndrewNg_2/A6428B3E31F2C2ED50CED904124A04F6.jpg" alt="IMAGE"><br>$\theta_0 = 360 $ $\theta_1 = 0$时，$ h_\theta (x)$ 如上图所示。</p>
<p><img src="/images/MachineLearning_AndrewNg_2/60EBCF577626A935F9FEC321BE324440.jpg" alt="IMAGE"><br>等高线越往中心中心，$J(\theta_0,\theta_1)$值越小，对应的假设函数越好。</p>
<h1 id="5-Gradient-Descent-梯度下降"><a href="#5-Gradient-Descent-梯度下降" class="headerlink" title="5 Gradient Descent 梯度下降"></a>5 Gradient Descent 梯度下降</h1><p>用梯度下降算法最小化代价函数J<br>已知函数 $J(\theta_0,\theta_1)$<br>想要 $\underset{\theta_0 ,\theta_1}{min}J(\theta_0,\theta_1)$</p>
<h2 id="梯度下降的思路"><a href="#梯度下降的思路" class="headerlink" title="梯度下降的思路"></a>梯度下降的思路</h2><ul>
<li>给定 $\theta_0,\theta_1$ 初始值，通常初始化为$\theta_0 = 0,\theta_1 = 0$</li>
<li>调整$\theta_0,\theta_1$使$J(\theta_0,\theta_1)$减小，直到我们减小到一个希望的最小值</li>
</ul>
<p>环顾四周，尽快下山要往哪个方向走？——找最佳的下山方向，即下降最快的方向<br><img src="/images/MachineLearning_AndrewNg_2/11946CF34AF9427A65A82096AD9ADAF9.jpg" alt="IMAGE"><br>每一步都环顾四周找最佳的下山方向，直到收敛converge至局部最低点。</p>
<p><strong>不同的起始位置，可能得到不同的局部最优解local optimum</strong><br><img src="/images/MachineLearning_AndrewNg_2/0EA4FB989F19304321231160FEC88A12.jpg" alt="IMAGE"></p>
<h2 id="梯度下降的方法"><a href="#梯度下降的方法" class="headerlink" title="梯度下降的方法"></a>梯度下降的方法</h2><p>repeat until covergence{<br>    $\quad \theta _j := \theta _j - \alpha \frac{\partial }{\partial \theta  _j}J(\theta_0,\theta_1)$ (for $j = 0$ and $j = 1$)<br>}</p>
<p>$a:=b$ assignment，赋值运算符，使a等于b值<br>$\alpha$ learning rate，学习率，控制梯度下降时迈出多大的步子，即上图中每个星号之间的距离。值很大时，梯度下降得迅速。</p>
<p>对于上式，需要同时更新 $\theta_0$ 和 $\theta_1$。<br><strong>simultaneous update正确方法：</strong><br>$temp0 := \theta _0 - \alpha \frac{\partial }{\partial \theta  _0}J(\theta_0,\theta_1)$<br>$temp1 := \theta _1 - \alpha \frac{\partial }{\partial \theta  _1}J(\theta_0,\theta_1)$<br>$\theta _0 := temp0$<br>$\theta _1 := temp1$</p>
<p>不正确方法：<br>$temp0 := \theta _0 - \alpha \frac{\partial }{\partial \theta  _0}J(\theta_0,\theta_1)$<br>$\theta _0 := temp0$<br>$temp1 := \theta _1 - \alpha \frac{\partial }{\partial \theta  _1}J(\theta_0,\theta_1)$<br>$\theta _1 := temp1$<br>没有做到同时更新，因为第三步计算 $temp1$ 时使用了新 的$\theta _0$。</p>
<h1 id="6-Gradient-Descent-intuition"><a href="#6-Gradient-Descent-intuition" class="headerlink" title="6 Gradient Descent intuition"></a>6 Gradient Descent intuition</h1><h2 id="导数项的意义"><a href="#导数项的意义" class="headerlink" title="导数项的意义"></a>导数项的意义</h2><p>简化为只有一个参数 $\theta_1$：<br>Repeat until convergence{<br>     $\quad \theta _1 := \theta _1 - \alpha \frac{\partial }{\partial \theta  _1}J(\theta_1)$<br>}</p>
<p>$\alpha$ 学习率始终是正数。</p>
<p>1.当 $\theta _1$ 在对称轴右侧<br><img src="/images/MachineLearning_AndrewNg_2/835A08F5195B5A9CFBB4307AF812A3C5.jpg" alt="IMAGE"></p>
<p>当 $\theta _1$ 在对称轴右侧，导数项 derivative term 即斜率 slope 为负数，因此新 $\theta _1$ 比之前小，向对称轴方向移动，最终收敛到函数的最小值。</p>
<p>2.当 $\theta _1$ 在对称轴左侧<br><img src="/images/MachineLearning_AndrewNg_2/9B517673316F8E106BBC84BBA438184E.jpg" alt="IMAGE"></p>
<p>当 $\theta _1$ 在对称轴左侧，导数项即斜率为正数，因此新 $\theta _1$ 比之前大，向对称轴方向移动，最终收敛到函数的最小值。</p>
<h2 id="alpha-的意义"><a href="#alpha-的意义" class="headerlink" title="$\alpha$ 的意义"></a>$\alpha$ 的意义</h2><p><img src="/images/MachineLearning_AndrewNg_2/2D3855C9C6833650C417EEBF13B4B138.jpg" alt="IMAGE"></p>
<p>如果 $\alpha$ 很小，梯度下降得慢。</p>
<p>如果 $\alpha$ 太大，可能一次次地越过最低点，会导致无法收敛甚至发散diverge。<br>当出现没有收敛或用了太多时间来获得最小值的情况时，意味着$\alpha$是错误的。</p>
<p>如果此时 $\theta _1$ 已经在一个局部最优点，下一步梯度下降会怎样？</p>
<div align=center>
  <img width=585 src="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/96EC02C8FF28F0AFEE43C3EDF51E8449.jpg" >
</div>

<p>此时导数为 0 ，$\theta _1$不再改变。</p>
<p>即使 $\alpha$ 保持不变，梯度下降法也能收敛到局部最低点。</p>
<p><img src="/images/MachineLearning_AndrewNg_2/A2412BF9070241E428678E7797C242A4.jpg" alt="IMAGE"><br>梯度下降的过程中，导数项自动变得越来越小，$\theta _1$更新的幅度也会越来越小。</p>
<h1 id="7-Gradient-Descent-For-Linear-Regression-线性回归的梯度下降"><a href="#7-Gradient-Descent-For-Linear-Regression-线性回归的梯度下降" class="headerlink" title="7 Gradient Descent For Linear Regression 线性回归的梯度下降"></a>7 Gradient Descent For Linear Regression 线性回归的梯度下降</h1><h2 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h2><p>用梯度下降和代价函数结合得到线性回归算法，用直线模型拟合数据。<br><img src="/images/MachineLearning_AndrewNg_2/D7374612F612FE96DB8B3C1C9AC0F33F.jpg" alt="IMAGE"><br>线性回归模型包括：线性假设linear hypothesis  以及 平方差代价函数 squared error cost function </p>
<p>接下来要将梯度下降算法应用于最小化平方差代价函数。</p>
<p>$$ \begin{align*}\frac{\partial }{\partial \theta  _j}J(\theta_0,\theta_1)<br> &amp;= \frac{\partial }{\partial \theta  <em>j} \frac{1}{2m}\sum</em>{i=1}^{m}\left ( h_\theta (x^{(i)}) - y^{(i)} \right )^2\<br> &amp;= \frac{\partial }{\partial \theta  <em>j} \frac{1}{2m}\sum</em>{i=1}^{m}\left ( \theta_0 + \theta_1x^{(i)}- y^{(i)} \right )^2\<br>\end{align*}$$</p>
<p>$j = 0$ 时，$\frac{\partial }{\partial \theta  <em>0}J(\theta_0,\theta_1)=  \frac{1}{m}\sum</em>{i=1}^{m}\left ( h_\theta (x^{(i)}) - y^{(i)} \right )$</p>
<p>$j = 1$ 时，$\frac{\partial }{\partial \theta  <em>1}J(\theta_0,\theta_1)=  \frac{1}{m}\sum</em>{i=1}^{m}\left ( h_\theta (x^{(i)}) - y^{(i)} \right )\cdot x^{(i)}$</p>
<p>梯度下降算法为</p>
<p>repeat until covergence{<br>    $\quad \theta _0 := \theta <em>0 - \alpha \frac{1}{m}\sum</em>{i=1}^{m}\left ( h_\theta (x^{(i)}) - y^{(i)} \right )$<br>    $\quad \theta _1 := \theta <em>1 - \alpha \frac{1}{m}\sum</em>{i=1}^{m}\left ( h_\theta (x^{(i)}) - y^{(i)} \right )\cdot x^{(i)}$<br>}<br>注意同时更新$\theta _0 $$\theta _1 $。</p>
<h2 id="可视化梯度下降过程"><a href="#可视化梯度下降过程" class="headerlink" title="可视化梯度下降过程"></a>可视化梯度下降过程</h2><div align=center>
  <img width=565 src="https://vikingstudyhard.github.io/images/MachineLearning_AndrewNg_2/69DB3FC7ED45D339810C543A4AB0D8FC.jpg" >
</div>

<p>通常线性回归的代价函数是凸函数 convex function，没有很多局部最优解，只有一个全剧最优解。</p>
<p>初始化为$\theta _0=900$,$\theta _1=-0.1$<br><img src="/images/MachineLearning_AndrewNg_2/6B66672C792589971296FDBC8640B2C0.jpg" alt="IMAGE"><br>此时假设函数为$h(x)=900-0.1x$<br>梯度下降到第二个点，假设函数也发生了变化：<br><img src="/images/MachineLearning_AndrewNg_2/731B07C8067BC6AF6B71C7A0BE4C4F5E.jpg" alt="IMAGE"><br>梯度下降到第三个点：<br><img src="/images/MachineLearning_AndrewNg_2/EA8EA41E04D01BCE2B944EB65CDB99EA.jpg" alt="IMAGE"><br>梯度下降到第四个点：<br><img src="/images/MachineLearning_AndrewNg_2/18C1EED4373417F265F9F4A4052E30B7.jpg" alt="IMAGE"><br>……<br>最后到达全局最小值点：<br><img src="/images/MachineLearning_AndrewNg_2/26BB6DF735AC62BC2FFAADD02564C522.jpg" alt="IMAGE"><br>得到了很好地拟合数据的假设函数，可以用来做预测。</p>
<h2 id="Batch-梯度下降"><a href="#Batch-梯度下降" class="headerlink" title="Batch 梯度下降"></a>Batch 梯度下降</h2><p>称为 Batch gradient descent：每一步梯度下降都遍历了整个训练集的样本，因为有这一项$\sum_{i=1}^{m}\left ( h_\theta (x^{(i)}) - y^{(i)} \right )$需要将所有预测值和实际值做差求和。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Andrew-Ng/" rel="tag"># Andrew Ng</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/01/17/MachineLearning_AndrewNg_1/" rel="prev" title="【Andrew Ng】Machine Learning | 1 Introdution">
      <i class="fa fa-chevron-left"></i> 【Andrew Ng】Machine Learning | 1 Introdution
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/01/19/MachineLearning_AndrewNg_3/" rel="next" title="【Andrew Ng】Machine Learning | 3 Linear Algebra review">
      【Andrew Ng】Machine Learning | 3 Linear Algebra review <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Model-representation-%E6%A8%A1%E5%9E%8B%E6%8F%8F%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">1 Model representation 模型描述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86"><span class="nav-number">1.1.</span> <span class="nav-text">训练集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%81%87%E8%AE%BE%E5%87%BD%E6%95%B0-hypothesis-function"><span class="nav-number">1.2.</span> <span class="nav-text">假设函数 hypothesis function</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-Cost-function-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">2.</span> <span class="nav-text">2 Cost function 代价函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E6%84%8F%E4%B9%89"><span class="nav-number">2.1.</span> <span class="nav-text">代价函数意义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC"><span class="nav-number">2.2.</span> <span class="nav-text">代价函数公式推导</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E5%AE%9A%E4%B9%89"><span class="nav-number">2.3.</span> <span class="nav-text">代价函数定义</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Cost-function-intuition-I"><span class="nav-number">3.</span> <span class="nav-text">3 Cost function intuition I</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E4%B8%80%E5%8F%82%E6%95%B0%E4%B8%8B%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E4%B8%8E%E5%81%87%E8%AE%BE%E5%87%BD%E6%95%B0%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-number">3.1.</span> <span class="nav-text">单一参数下代价函数与假设函数的关系</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-Cost-function-intuition-II"><span class="nav-number">4.</span> <span class="nav-text">4 Cost function intuition II</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8C%E5%8F%82%E6%95%B0%E4%B8%8B%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E4%B8%8E%E5%81%87%E8%AE%BE%E5%87%BD%E6%95%B0%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-number">4.1.</span> <span class="nav-text">双参数下代价函数与假设函数的关系</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-Gradient-Descent-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">5.</span> <span class="nav-text">5 Gradient Descent 梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%80%9D%E8%B7%AF"><span class="nav-number">5.1.</span> <span class="nav-text">梯度下降的思路</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">5.2.</span> <span class="nav-text">梯度下降的方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-Gradient-Descent-intuition"><span class="nav-number">6.</span> <span class="nav-text">6 Gradient Descent intuition</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%BC%E6%95%B0%E9%A1%B9%E7%9A%84%E6%84%8F%E4%B9%89"><span class="nav-number">6.1.</span> <span class="nav-text">导数项的意义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#alpha-%E7%9A%84%E6%84%8F%E4%B9%89"><span class="nav-number">6.2.</span> <span class="nav-text">$\alpha$ 的意义</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7-Gradient-Descent-For-Linear-Regression-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">7.</span> <span class="nav-text">7 Gradient Descent For Linear Regression 线性回归的梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC"><span class="nav-number">7.1.</span> <span class="nav-text">公式推导</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%BF%87%E7%A8%8B"><span class="nav-number">7.2.</span> <span class="nav-text">可视化梯度下降过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">7.3.</span> <span class="nav-text">Batch 梯度下降</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="VikingWang"
      src="/images/avatar.JPG">
  <p class="site-author-name" itemprop="name">VikingWang</p>
  <div class="site-description" itemprop="description">这是一个需要耐心等着加载的博客</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">39</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">VikingWang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
